{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Variational Inference and Generative Modelling Tutorial</h1>\n",
    "    <h3> Weizmann AI Hub for Scientific Discovery </h3>\n",
    "    <h4>Nathan LEVY</h4>\n",
    "    nathan.levy@weizmann.ac.il\n",
    "    <p>with inputs from N.Yosef and M.Kim</p>\n",
    "    <p>Spring semester 2024</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to delve into a class of generative models called Variational Auto-Encoders. We start by stating key statistical concepts before deriving the relevant mathematical formulation. Then we build and train a VAE. \\\n",
    "Finally, we are going to apply the models we built to single-cell RNA-seq data, and show how these models can be used for representation learning.\n",
    "\n",
    "ðŸ’¡ This tutorial does not assume prior knowledge in variational inference or biology, but does rely on basic deep learning skills, such as building and training a feedforward neural network. AI hub students should use the `scvi-env` environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "To begin with, let us review a few useful ML concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Discriminative versus Generative learning\n",
    "\n",
    "\n",
    "**Discriminative Learning:** \\\n",
    "Discriminative learning is a machine learning approach that focuses on learning the boundary or decision boundary that separates different classes or categories within the data. The primary goal of discriminative models is to find a function that directly maps the input data to the corresponding class labels. These models are designed to distinguish between different classes based on their distinct features or patterns.\n",
    "\n",
    "A basic example of discriminative model is logistic regression, it is often used in classification tasks where the goal is to assign input data points to specific predefined categories or classes.\n",
    "\n",
    "\n",
    "\n",
    "**Generative Learning:**\\\n",
    "Generative learning, on the other hand, is a machine learning approach that aims to model the underlying probability distribution of the entire dataset. Instead of focusing solely on learning the decision boundary, generative models attempt to capture the inherent structure and patterns within the data. This allows them to generate new samples that resemble the original data distribution.\n",
    "\n",
    "Generative models can be used for various tasks, including data synthesis, image generation, and anomaly detection. Some popular generative models include Gaussian Mixture Models (GMMs), Variational Autoencoders (VAEs), and Generative Adversarial Networks (GANs).\n",
    "\n",
    "In summary, discriminative learning only models the outputs given the inputs, while generative learning models the joint probability of inputs and outputs. \n",
    "\n",
    "\n",
    "The focus of this tutorial is on generative learning. To fit the generative models, we adopt the Bayesian inference paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Bayesian modelling and Variational Inference\n",
    "\n",
    "\n",
    "Suppose we have a probabilistic model with some parameters we want to estimate. The Bayesian paradigm involves the following key concepts:\n",
    "\n",
    "1. **Prior Probability:** Before observing any new data, we have an initial belief about our model parameters $\\theta$. This belief is expressed as the prior probability $p(\\theta)$.\n",
    "\n",
    "2. **Likelihood:** Given a dataset $D$, the likelihood function $L$ describes how likely the data is to occur given the underlying parameters: $L=p(D \\mid \\theta)$\n",
    "\n",
    "3. **Posterior Probability:** After observing new data, the Bayesian approach updates the initial beliefs to incorporate information from this data. This process is called *Bayesian inference*. The updated parameters are represented as the posterior probability $p(\\theta \\mid D)$, which combines the prior probability and the likelihood.\n",
    "\n",
    "4. **Evidence:** The term $p(D)$ represents the likelihood of observing the data across all possible values of the parameters or hypotheses. It acts as a normalizing constant to ensure that the posterior probability integrates to 1. The main goal of any Bayesian count_model is to optimize the evidence for the data, which is also known as the marginal likelihood. It is often more convenient to work with the log marginal likelihood.\n",
    "\n",
    "\n",
    "The posterior probability is computed via the *Bayes rule*:\n",
    "\n",
    "$$ p(\\theta \\mid D) = \\frac{p(D|Î¸)p(Î¸)}{\\int p(D|Î¸)p(Î¸)dÎ¸} = \\frac{p(D|Î¸)p(Î¸)}{p(D)}$$\n",
    "\n",
    "\n",
    "However, in most models, the posterior distribution is intractable because of the integral defining the evidence, which runs over the space of all $\\theta$. Therefore, the posterior must be estimated: this is called *approximate Bayesian inference*. \n",
    "\n",
    "There are a few popular approximate Bayesian inference methods, among which Markov Chain Monte Carlo (the idea is to construct a Markov chain that has the posterior distribution as its equilibrium distribution) and Variational Inference, which tries to minimize the distance between an approximate, tractable posterior (called the variational distribution) and the true one. We are going to focus on the latter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Latent Variable Models\n",
    "\n",
    "Given observed data $D$, Latent Variable Models assume that the data generating process is driven by unobserved *latent variables* $Z$, of smaller dimension. The latent variables should be a summary of the data, driving its variability. They can represent a useful representation of the data. \n",
    "\n",
    "\n",
    "Mixture Models are a classsic example of LVMs. They assume that the dataset $D$ is generated by sampling i.i.d. from $K$ distincts distributions called mixture components. We can describe the data generating process as follows: \n",
    "\n",
    "1. For each datapoint, we first sample the mixture assignment from a categorical random variable, out of $K$ possible components. \n",
    "2. Then, we sample from the assigned distribution. \n",
    "\n",
    "In this count_model, we do observe the datapoints, which were drawn conditionnally on the mixture assignment, but we do not observe the assignment itself: it is the latent variable. \n",
    "\n",
    "Given the observed data, we want to infer the latent variables, using the posterior distribution $p(Z \\mid D)$. For this purpose, we'll have to rely on approximate Bayesian inference, more precisely Variational Inference. Thus we introduce the approximate posterior $q(Z \\mid D)$, which comes from a family of tractable distributions. As we said, the goal is to minimize the distance between the true and approximate posterior. The common metric to use is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). It is defined as: \n",
    "$$\\mathbb{KL}(q \\mid p) = \\int q(x) \\log \\frac{q(x)}{p(x)} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In other words, it is a measure of how much information is lost when the second distribution is used to approximate the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 1</b>\n",
    "Is the KL divergence symmetric? What is its value when the two distributions are equal? What is its minimum value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence Lower Bound (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fit out latent variable count_model to the data, but the log marginal likelihood is intractable. We also want to evaluate the posterior distribution $p(Z \\mid D)$, but it is also intractable, as its denominator is the evidence. Instead, we are going to maximize a lower bound on the log marginal likelihood, called the Evidence Lower Bound (ELBO).\n",
    "\n",
    "Recall from Bayes rule that the log marginal likelihood can be written as:\n",
    "$$\\log p(D) = \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} $$\n",
    "\n",
    "We take the expectation with respect to the approximate posterior $q(Z \\mid D)$ - we write this operation $\\mathbb{E}_{Z \\sim q}[\\cdot]$, which gives the same result as integrating over $Z$ with respect to $q(Z \\mid D)$:\n",
    "\n",
    "$$\\log \\left[ p(D) \\right] = \\mathbb{E}_{Z \\sim q} \\left[ \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} \\right]$$\n",
    "\n",
    "Multiplying and dividing by $q(Z \\mid D)$, we get:\n",
    "\n",
    "$$\\log \\left[ p(D) \\right] = \\mathbb{E}_{Z \\sim q} \\left[ \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} \\frac {q(Z|D)}{q(Z|D)} \\right]$$\n",
    "\n",
    "Thus we can decompose the log marginal likelihood as follows:\n",
    "\n",
    "$$\\log p(D) = \\mathbb{E}_{Z \\sim q} \\log \\frac{q(Z|D)}{p(Z|D)} + \\mathbb{E}_{Z \\sim q} \\log \\frac{p(Z)}{q(Z|D)} + \\mathbb{E}_{Z \\sim q} \\log p(D | Z)$$\n",
    "\n",
    "This can be re-expressed using the KL divergence as follows:\n",
    "\n",
    "$$\\log p(D) = \\mathbb{KL}(q(Z | D) \\| p(Z | D)) - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "Because the KL divergence is always positive, we get the following inequality:\n",
    "$$\\log p(D) \\geq - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "The right-hand side of the inequality is called the Evidence Lower Bound [(ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound). It is a lower bound on the log marginal likelihood. By maximizing the ELBO, we avoid the intractability of the log marginal likelihood. \n",
    "\n",
    "$$ELBO =  - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "The ELBO includes two sets of parameters: the parameters  $\\phi$ of the approximate posterior $q$ and the parameters  $\\theta$ of likelihood $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Deep Generative Models (DGMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep generative models are generative models that assume that both distributions $q$ and $p$ are parameterized by neural networks.\n",
    "Variational Autoencoders (VAEs) are a class of DGMs with two key components: an encoder and a decoder. The encoder maps the input data to a latent representation, while the decoder maps the latent representation back to the input space. The encoder $f_\\phi$ and decoder $f_\\theta$ are trained jointly to maximize the ELBO.\n",
    "\n",
    "A typical choice for the latent prior $p(Z)$ is the standard isotropic Gaussian $\\mathcal{N}(0,I)$, while the appoximate posterior can be chosen as diagonal Gaussian\n",
    "$$q(Z \\mid D) = \\mathcal{N}(\\mu_\\phi(D), \\sigma_\\phi(D))$$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$\\mu_\\phi(D), \\sigma_\\phi(D) = f_\\phi(D)$$\n",
    "\n",
    "Where $f_\\phi$ is a neural network with parameters $\\phi$ and $\\sigma_\\phi(D)$ is a diagonal covariance matrix with zero off-diagonal elements.\n",
    "\n",
    "With these choices, the KL divergence term in the ELBO can be computed in closed form.\n",
    "The form of decoder distribution changes with the type of data we want to model. For example, if we want to model **binary data**, we can use a Bernoulli distribution for the decoder: \n",
    "$$p(D \\mid Z) = \\mathcal{B}(\\mu_\\theta(Z))$$\n",
    "With $\\mu_\\theta(Z) = f_\\theta(Z)$.\n",
    "\n",
    "If we want to model **continuous data**, we can use a (diagonal) Gaussian distribution for the decoder, such that:\n",
    "$$p(D \\mid Z) = \\mathcal{N}(\\mu_\\theta(Z), \\sigma_\\theta(Z))$$\n",
    "With $\\mu_\\theta(Z), \\sigma_\\theta(Z) = f_\\theta(Z)$.\n",
    "\n",
    "Finally, for **count data**, such as gene expression counts in single-cell RNA sequencing, a Poisson distribution is often an appropriate choice for the decoder. In this case, the decoder distribution can be modeled as:\n",
    "\n",
    "$$p(D \\mid Z) = \\text{Poisson}(\\lambda_\\theta(Z))$$\n",
    "\n",
    "Where $\\lambda_\\theta(Z) = f_\\theta(Z)$ represents the rate parameter of the Poisson distribution. This parameter is typically interpreted as the expected count or mean of the distribution, and it is always non-negative. The function $f_\\theta(Z)$ is usually implemented as a neural network that maps the latent space Z to the positive real numbers, often using an exponential or softplus activation function in the final layer to ensure non-negativity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 3 distributions above, we can analytically compute the two ELBO terms: the KL $\\mathbb{KL}(q(Z | D) \\| p(Z))$ and the reconstruction term $\\mathbb{E}_{Z \\sim q} \\log p(D|Z)$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 2</b>\n",
    "What is the analytical expression of the KL divergence? What is the analytical expression of the reconstruction term for a Gaussian decoder? For a Bernoulli decoder?\n",
    "\n",
    "ðŸ’¡ Hints: minimizing the negative Bernoulli log-likelihood is equivalent to minimizing the binary cross-entropy loss. Minimizing the negative Gaussian log-likelihood is equivalent to minimizing the mean squared error for unit variance. You may also check the VAE seminal paper https://arxiv.org/pdf/1312.6114v10.pdf. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic optimization of the ELBO\n",
    "\n",
    "We want to use stochastic gradient descent to optimize the ELBO. For this purpose, we need to compute the gradient of the ELBO with respect to the parameters $\\phi$ and $\\theta$. However, we cannot differentiate the ELBO w.r.t. $\\phi$ as we cannot directly backpropagate\n",
    "gradients through the random variable $Z$. We will need to use a change of variables called the *reparametrization trick*. Please read about the trick [here](https://sassafras13.github.io/ReparamTrick/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Design a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we derived the mathematical formulation of the VAE, we are going to implement it using PyTorch. The code to complete is contained in `base_components.py`, `model.py` and `train.py`. Utility functions are provided in `utils.py`. All files are auto-imported in the notebook. It means that you can change them, save them (`Ctrl+S`) and this change will immediately take effect in the notebook.\n",
    "\n",
    "We focus on single-cell RNA-seq data. The data consists in  a matrix of gene expression counts, where each row corresponds to a cell and each column corresponds to a gene. The goal is to learn a low-dimensional representation of the cells, which captures the main sources of variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build the Encoder and the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General instructions:**\n",
    "\n",
    "- You are free to use whatever you prefer to build the model layers, but we provide you with a helper function `make_layer`, with default ReLU and BatchNorm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example usage of make_layer\n",
    "\n",
    "# from base_components import make_layer\n",
    "\n",
    "# input_size = 784\n",
    "# hidden_sizes = [256, 128]\n",
    "# output_size = 2\n",
    "\n",
    "# layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "\n",
    "# layers = torch.nn.Sequential(\n",
    "#     *[\n",
    "#         make_layer(\n",
    "#             in_dim=in_size,\n",
    "#             out_dim=out_size,\n",
    "#         )\n",
    "#         for (in_size, out_size) in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either code the reparameterization trick or use the rsample method of torch.distributions.Normal. This method returns a sample from the distribution, with gradients flowing through the mean and standard deviation. You can have a look at the documentation [here](https://pytorch.org/docs/stable/distributions.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 3</b>\n",
    "Complete the code in `base_components.py` to build the encoder and the decoder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Build the VAE model\n",
    "\n",
    "- The VAE model is built by combining the encoder and the decoder.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 4</b>\n",
    "Complete the code in `model.py` to build the VAE model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Applying DGMs to sc-RNA-seq data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup (change the params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "LATENT_DIM = 1\n",
    "N_HIDDEN = 1\n",
    "MAX_KL_WEIGHT = 1\n",
    "N_EPOCHS = 1\n",
    "LR = 1\n",
    "BATCH_KEY = \"batch\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the human lung cell atlas introduced in [Luecken et al. 2021](https://www.nature.com/articles/s41592-021-01336-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\n",
    "    \"lung_atlas.h5ad\",\n",
    "    backup_url=\"https://figshare.com/ndownloader/files/24539942\",\n",
    ")\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts\"]  # .todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing\n",
    "\n",
    "We perform basic preprocessing on the sc-RNA-seq data. We filter out lowly expressed genes. In order to compute a PCA later on, we normalize and log-transform the data. \n",
    "\n",
    "ðŸ’¡ More details on these steps can be found on the single-cell integration tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_counts=5)\n",
    "sc.pp.filter_cells(adata, min_counts=5)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    n_top_genes=1200,\n",
    "    subset=True,\n",
    "    layer=\"counts\",\n",
    "    flavor=\"seurat_v3\",\n",
    "    batch_key=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataloader\n",
    "\n",
    "We use [AnnDataLoader](https://docs.scvi-tools.org/en/stable/tutorials/notebooks/dev/data_tutorial.html#recording-anndata-state-with-object-registration) from scvi-tools to build the dataloader. We will select the subsets of the AnnData fthat are needed for training and inference. Of course we'll need to load the counts. If you want to build a conditional VAE, you'll also need to load the batch and/or the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.data import AnnDataManager\n",
    "from scvi.data.fields import CategoricalObsField, LayerField\n",
    "from scvi.dataloaders import AnnDataLoader\n",
    "\n",
    "\n",
    "# which fields of th AnnData do you need?\n",
    "anndata_fields = [\n",
    "    LayerField(registry_key=\"counts\", layer=\"counts\", is_count_data=True),  # raw counts\n",
    "    CategoricalObsField(\n",
    "        registry_key=\"batch\", attr_key=\"batch\"\n",
    "    ),  # if needed: catagorical field in adata.obs for the batch info\n",
    "    CategoricalObsField(\n",
    "        registry_key=\"cell_type\", attr_key=\"cell_type\"\n",
    "    ),  # if needed: catagorical field in adata.obs for the cell type label info\n",
    "]\n",
    "\n",
    "adata_manager = AnnDataManager(fields=anndata_fields)\n",
    "adata_manager.register_fields(adata)\n",
    "\n",
    "# print out the summary stats of the data: how many batches, cell types, genes..\n",
    "adata_manager.summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = len(adata)\n",
    "frac_train = 0.8\n",
    "\n",
    "\n",
    "# Randomly split the cells into training and testing sets\n",
    "train_index = np.random.choice(n_cells, size=int(frac_train * n_cells), replace=False)\n",
    "train_mask = np.empty(shape=(n_cells,), dtype=bool)\n",
    "train_mask[:] = False\n",
    "train_mask[train_index] = True\n",
    "val_mask = np.logical_not(train_mask)\n",
    "\n",
    "print(f\"{train_mask.sum()} training cells\")\n",
    "print(f\"{val_mask.sum()} validation cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an AnnDataLoader which will iterate over our anndata\n",
    "adl_train = AnnDataLoader(\n",
    "    adata_manager, shuffle=False, batch_size=BATCH_SIZE, indices=train_index\n",
    ")\n",
    "adl_validation = AnnDataLoader(\n",
    "    adata_manager, shuffle=False, batch_size=BATCH_SIZE, indices=val_mask\n",
    ")\n",
    "\n",
    "\n",
    "# get the first batch of data\n",
    "ex_data_batch = next(iter(adl_train))\n",
    "\n",
    "\n",
    "print(ex_data_batch[\"counts\"].shape)  # shape is batch_size x n_genes\n",
    "print(ex_data_batch[\"batch\"].shape)  # shape is batch_size x 1\n",
    "print(ex_data_batch[\"cell_type\"].shape)  # shape is batch_size x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VAE\n",
    "\n",
    "# count_model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 5</b>\n",
    "Complete the code in `train.py` to train the model. You should use the analytical expressions of the KL divergence and the reconstruction term that you derived in Question 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common practise is to warmup the KL divergence term in the ELBO. This is done by multiplying the KL term by a weight, which is gradually increased from 0 to 1 over the course of training. This is done to prevent the model from learning a trivial latent representation, as discussed in [this paper](https://arxiv.org/pdf/1602.02282.pdf).\n",
    "\n",
    "A `_compute_kl_weight` function is provided in `utils.py` to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_loop\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR  # try more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try different architectures and hyperparameters. \n",
    "Try different optimizers and schedulers - as listed in [torch.optim](https://pytorch.org/docs/stable/optim.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer =\n",
    "\n",
    "# scheduler =\n",
    "\n",
    "\n",
    "# train_kl, train_recon, test_kl, test_recon = train_loop(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_kl, label=\"Train KL\")\n",
    "# plt.plot(test_kl, label=\"Val KL\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_recon, label=\"Train Recon\")\n",
    "# plt.plot(test_recon, label=\"Val Recon\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Get the latent representation, compare with PCA\n",
    "\n",
    "Once the model is trained, we can use the encoder to get the latent representation of the cells. We can then compare the VAE latent representation with the PCA representation.\n",
    "\n",
    "Compared to the VAE, the PCA is a linear method that projects the data into a lower-dimensional space by maximizing the variance of the projected data. It is a deterministic method that does not model the data distribution. You may refresh your memory on PCA [here](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 6</b>\n",
    "Implement the `get_latent_representation` function in `model.py` to get the latent representation of the cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will visualize the latent and PCA spaces using UMAP - a dimensionality reduction technique that is often used in single-cell analysis to visualize the data in 2D or 3D.\n",
    "If you are not familiar with UMAP, you can read more about it [here](https://www.scdiscoveries.com/blog/knowledge/what-is-a-umap-plot/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_latent_representation\n",
    "\n",
    "# adata.obsm[\"X_vae\"] = get_latent_representation(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"X_vae\")\n",
    "sc.tl.umap(adata, min_dist=0.3)\n",
    "\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[\"cell_type\"],\n",
    "    frameon=False,\n",
    ")\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[\"donor\", BATCH_KEY],\n",
    "    ncols=2,\n",
    "    frameon=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA then generate UMAP plots\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, n_pcs=30, n_neighbors=20)\n",
    "sc.tl.umap(adata, min_dist=0.3)\n",
    "\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[\"cell_type\"],\n",
    "    frameon=False,\n",
    ")\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[\"donor\", BATCH_KEY],\n",
    "    ncols=2,\n",
    "    frameon=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Benchmark with scIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Now we can provide quantitative measurement of the integration performance, using metrics developped in the paper [Benchmarking atlas-level data integration in single-cell genomics](https://www.nature.com/articles/s41592-021-01336-8).\n",
    "\n",
    "Scalable implementation of these metrics are available in the [scib-metrics](https://scib-metrics.readthedocs.io/en/stable/index.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scib_metrics.benchmark import Benchmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a set of metrics using scib.\n",
    "\n",
    "\n",
    "bm = Benchmarker(\n",
    "    adata,\n",
    "    batch_key=\"batch\",\n",
    "    label_key=\"cell_type\",\n",
    "    embedding_obsm_keys=[\"X_pca\", \"X_vae\"],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "bm.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.plot_results_table(min_max_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should aim for a better performance than PCA in both Batch correction and Bio Conservation metrics. Find the set of parameters/hyperparameters that gives you the best performance ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Bonus: Implement batch correction in the VAE\n",
    "\n",
    "The idea here is to concatenate to the decoder imput the batch information of the cells. This can be done by adding a batch [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to the decoder. The batch embedding layer is a learnable matrix of size (n_batches, n_latent). Alternatively, you can embedd the batch indices using [one-hot](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) vectors. The batch information is then concatenated to the latent representation before feeding it to the decoder. This is a simple way to correct for batch effects in the data. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 7</b>\n",
    "Modify the decoder to include batch information. Train the model and compare the latent representations with and without batch correction using the scIB metrics.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenospace_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
