{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Variational Inference and Generative Modelling Tutorial</h1>\n",
    "    <h3> Weizmann AI Hub for Scientific Discovery </h3>\n",
    "    <h4>Nathan LEVY</h4>\n",
    "    nathan.levy@weizmann.ac.il\n",
    "    <p>with inputs from N.Yosef</p>\n",
    "    <p>Winter semester 2023</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to delve into a class of generative count_count_count_models called Variational Auto-Encoders. We start by stating key statistical concepts before deriving the relevant mathematical formulation. Then we build and train a VAE. \\\n",
    "Finally, we are going to apply the models we built to single-cell RNA-seq data, and show how these models can be used for representation learning.\n",
    "\n",
    "ðŸ’¡ This tutorial does not assume prior knowledge in variational inference or biology, but does rely on basic deep learning skills, such as building and training a feedforward neural network. AI hub students should use the `scvi-env` environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To begin with, let us review a few useful ML concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative versus Generative learning\n",
    "\n",
    "\n",
    "**Discriminative Learning:** \\\n",
    "Discriminative learning is a machine learning approach that focuses on learning the boundary or decision boundary that separates different classes or categories within the data. The primary goal of discriminative models is to find a function that directly maps the input data to the corresponding class labels. These models are designed to distinguish between different classes based on their distinct features or patterns.\n",
    "\n",
    "A basic example of discriminative model is logistic regression, it is often used in classification tasks where the goal is to assign input data points to specific predefined categories or classes.\n",
    "\n",
    "\n",
    "\n",
    "**Generative Learning:**\\\n",
    "Generative learning, on the other hand, is a machine learning approach that aims to model the underlying probability distribution of the entire dataset. Instead of focusing solely on learning the decision boundary, generative models attempt to capture the inherent structure and patterns within the data. This allows them to generate new samples that resemble the original data distribution.\n",
    "\n",
    "Generative models can be used for various tasks, including data synthesis, image generation, and anomaly detection. Some popular generative models include Gaussian Mixture Models (GMMs), Variational Autoencoders (VAEs), and Generative Adversarial Networks (GANs).\n",
    "\n",
    "In summary, discriminative learning only models the outputs given the inputs, while generative learning models the joint probability of inputs and outputs. \n",
    "\n",
    "\n",
    "The focus of this tutorial is on generative learning. To fit the generative models, we adopt the Bayesian inference paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian modelling and Variational Inference\n",
    "\n",
    "\n",
    "Suppose we have a probabilistic model with some parameters we want to estimate. The Bayesian paradigm involves the following key concepts:\n",
    "\n",
    "1. **Prior Probability:** Before observing any new data, we have an initial belief about our model parameters $\\theta$. This belief is expressed as the prior probability $p(\\theta)$.\n",
    "\n",
    "2. **Likelihood:** Given a dataset $D$, the likelihood function $L$ describes how likely the data is to occur given the underlying parameters: $L=p(D \\mid \\theta)$\n",
    "\n",
    "3. **Posterior Probability:** After observing new data, the Bayesian approach updates the initial beliefs to incorporate information from this data. This process is called *Bayesian inference*. The updated parameters are represented as the posterior probability $p(\\theta \\mid D)$, which combines the prior probability and the likelihood.\n",
    "\n",
    "4. **Evidence:** The term $p(D)$ represents the likelihood of observing the data across all possible values of the parameters or hypotheses. It acts as a normalizing constant to ensure that the posterior probability integrates to 1. The main goal of any Bayesian count_model is to optimize the evidence for the data, which is also known as the marginal likelihood. It is often more convenient to work with the log marginal likelihood.\n",
    "\n",
    "\n",
    "The posterior probability is computed via the *Bayes rule*:\n",
    "\n",
    "$$ p(\\theta \\mid D) = \\frac{p(D|Î¸)p(Î¸)}{\\int p(D|Î¸)p(Î¸)dÎ¸} = \\frac{p(D|Î¸)p(Î¸)}{p(D)}$$\n",
    "\n",
    "\n",
    "However, in most models, the posterior distribution is intractable because of the integral defining the evidence, which runs over the space of all $\\theta$. Therefore, the posterior must be estimated: this is called *approximate Bayesian inference*. \n",
    "\n",
    "There are a few popular approximate Bayesian inference methods, among which Markov Chain Monte Carlo (the idea is to construct a Markov chain that has the posterior distribution as its equilibrium distribution) and Variational Inference, which tries to minimize the distance between an approximate, tractable posterior (called the variational distribution) and the true one. We are going to focus on the latter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Variable Models\n",
    "\n",
    "Given observed data $D$, Latent Variable Models assume that the data generating process is driven by unobserved *latent variables* $Z$, of smaller dimension. The latent variables should be a summary of the data, driving its variability. They can represent a useful representation of the data. \n",
    "\n",
    "\n",
    "Mixture Models are a classsic example of LVMs. They assume that the dataset $D$ is generated by sampling i.i.d. from $K$ distincts distributions called mixture components. We can describe the data generating process as follows: \n",
    "\n",
    "1. For each datapoint, we first sample the mixture assignment from a categorical random variable, out of $K$ possible components. \n",
    "2. Then, we sample from the assigned distribution. \n",
    "\n",
    "In this count_model, we do observe the datapoints, which were drawn conditionnally on the mixture assignment, but we do not observe the assignment itself: it is the latent variable. \n",
    "\n",
    "Given the observed data, we want to infer the latent variables, using the posterior distribution $p(Z \\mid D)$. For this purpose, we'll have to rely on approximate Bayesian inference, more precisely Variational Inference. Thus we introduce the approximate posterior $q(Z \\mid D)$, which comes from a family of tractable distributions. As we said, the goal is to minimize the distance between the true and approximate posterior. The common metric to use is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). It is defined as: \n",
    "$$\\mathbb{KL}(q \\mid p) = \\int q(x) \\log \\frac{q(x)}{p(x)} dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In other words, it is a measure of how much information is lost when the second distribution is used to approximate the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 1</b>\n",
    "Is the KL divergence symmetric? What is its value when the two distributions are equal? What is its minimum value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence Lower Bound (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fit out latent variable count_model to the data, but the log marginal likelihood is intractable. We also want to evaluate the posterior distribution $p(Z \\mid D)$, but it is also intractable, as its denominator is the evidence. Instead, we are going to maximize a lower bound on the log marginal likelihood, called the Evidence Lower Bound (ELBO).\n",
    "\n",
    "Recall from Bayes rule that the log marginal likelihood can be written as:\n",
    "$$\\log p(D) = \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} = \\mathbb{E}_{Z \\sim q} \\left[ \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} \\right]$$\n",
    "\n",
    "Taking the expectation with respect to the approximate posterior $q(Z \\mid D)$, we get:\n",
    "\n",
    "$$\\log \\left[ p(D) \\right] = \\mathbb{E}_{Z \\sim q} \\left[ \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} \\right]$$\n",
    "\n",
    "Multiplying and dividing by $q(Z \\mid D)$, we get:\n",
    "\n",
    "$$\\log \\left[ p(D) \\right] = \\mathbb{E}_{Z \\sim q} \\left[ \\log\\frac{p(D \\mid Z)p(Z)}{p(Z \\mid D)} \\frac {q(Z|D)}{q(Z|D)} \\right]$$\n",
    "\n",
    "Thus we can decompose the log marginal likelihood as follows:\n",
    "\n",
    "$$\\log p(D) = \\mathbb{E}_{Z \\sim q} \\log \\frac{q(Z|D)}{p(Z|D)} + \\mathbb{E}_{Z \\sim q} \\log \\frac{p(Z)}{q(Z|D)} + \\mathbb{E}_{Z \\sim q} \\log p(D | Z)$$\n",
    "\n",
    "This can be re-expressed using the KL divergence as follows:\n",
    "\n",
    "$$\\log p(D) = \\mathbb{KL}(q(Z | D) \\| p(Z | D)) - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "Because the KL divergence is always positive, we get the following inequality:\n",
    "$$\\log p(D) \\geq - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "The right-hand side of the inequality is called the Evidence Lower Bound [(ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound). It is a lower bound on the log marginal likelihood. By maximizing the ELBO, we avoid the intractability of the log marginal likelihood. \n",
    "\n",
    "$$ELBO =  - \\mathbb{KL}(q(Z | D) \\| p(Z)) + \\mathbb{E}_{Z \\sim q} \\log p(D|Z)$$\n",
    "\n",
    "The ELBO includes two sets of parameters: the parameters  $\\phi$ of the approximate posterior $q$ and the parameters  $\\theta$ of likelihood $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Generative Models (DGMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep generative models are generative models that assume that both distributions $q$ and $p$ are parameterized by neural networks.\n",
    "Variational Autoencoders (VAEs) are a class of DGMs with two key components: an encoder and a decoder. The encoder maps the input data to a latent representation, while the decoder maps the latent representation back to the input space. The encoder $f_\\phi$ and decoder $f_\\theta$ are trained jointly to maximize the ELBO.\n",
    "\n",
    "A typical choice for the latent prior $p(Z)$ is the standard isotropic Gaussian $\\mathcal{N}(0,I)$, while the appoximate posterior can be chosen as diagonal Gaussian\n",
    "$$q(Z \\mid D) = \\mathcal{N}(\\mu_\\phi(D), \\sigma_\\phi(D))$$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$\\mu_\\phi(D), \\sigma_\\phi(D) = f_\\phi(D)$$\n",
    "\n",
    "Where $f_\\phi$ is a neural network with parameters $\\phi$ and $\\sigma_\\phi(D)$ is a diagonal covariance matrix with zero off-diagonal elements.\n",
    "\n",
    "With these choices, the KL divergence term in the ELBO can be computed in closed form.\n",
    "The form of decoder distribution changes with the type of data we want to model. For example, if we want to model binary data, we can use a Bernoulli distribution for the decoder: \n",
    "$$p(D \\mid Z) = \\mathcal{B}(\\mu_\\theta(Z))$$\n",
    "With $\\mu_\\theta(Z) = f_\\theta(Z)$.\n",
    "\n",
    "If we want to model continuous data, we can use a (diagonal) Gaussian distribution for the decoder, such that:\n",
    "$$p(D \\mid Z) = \\mathcal{N}(\\mu_\\theta(Z), \\sigma_\\theta(Z))$$\n",
    "With $\\mu_\\theta(Z), \\sigma_\\theta(Z) = f_\\theta(Z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 3 distributions above, we can analytically compute the two ELBO terms: the KL $\\mathbb{KL}(q(Z | D) \\| p(Z))$ and the reconstruction term $\\mathbb{E}_{Z \\sim q} \\log p(D|Z)$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 2</b>\n",
    "What is the analytical expression of the KL divergence? What is the analytical expression of the reconstruction term for a Gaussian decoder? For a Bernoulli decoder?\n",
    "\n",
    "ðŸ’¡ Hints: minimizing the negative Bernoulli log-likelihood is equivalent to minimizing the binary cross-entropy loss. Minimizing the negative Gaussian log-likelihood is equivalent to minimizing the mean squared error for unit variance. You may also check the VAE seminal paper https://arxiv.org/pdf/1312.6114v10.pdf. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic optimization of the ELBO\n",
    "\n",
    "We want to use stochastic gradient descent to optimize the ELBO. For this purpose, we need to compute the gradient of the ELBO with respect to the parameters $\\phi$ and $\\theta$. However, we cannot differentiate the ELBO w.r.t. $\\phi$ as we cannot directly backpropagate\n",
    "gradients through the random variable $Z$. We will need to use a change of variables called the *reparametrization trick*. Please read about the trick [here](https://sassafras13.github.io/ReparamTrick/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design and train a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we derived the mathematical formulation of the VAE, we are going to implement it. We will use the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits. You can find more about the dataset [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "\n",
    "The goal is to train a VAE to generate images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rich import print\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.<span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample from a Poisson distribution\n",
    "\n",
    "from torch.distributions import Poisson\n",
    "\n",
    "p = Poisson(torch.tensor([5.0]))\n",
    "\n",
    "\n",
    "print(p.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Encoder and the Decoder:\n",
    "\n",
    "\n",
    "To make use of the reparameterization trick, you should use the rsample method of torch.distributions.Normal. This method returns a sample from the distribution, with gradients flowing through the mean and standard deviation. You can have a look at the documentation [here](https://pytorch.org/docs/stable/distributions.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 3</b>\n",
    "Complete the code below to build the encoder and the decoder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to use whatever you prefer to build the model layers, but we provide you with a helper function, with default ReLU and BatchNorm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of make_layer\n",
    "\n",
    "# input_size = 784\n",
    "# hidden_sizes = [256, 128]\n",
    "# output_size = 2\n",
    "\n",
    "# layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "\n",
    "# layers = torch.nn.Sequential(\n",
    "#     *[\n",
    "#         make_layer(\n",
    "#             in_dim=in_size,\n",
    "#             out_dim=out_size,\n",
    "#         )\n",
    "#         for (in_size, out_size) in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 4</b>\n",
    "Complete the code below to train the model. You should use the analytical expressions of the KL divergence and the reconstruction term that you derived in Question 2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using cpu device\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using cpu device\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "LATENT_DIM = 10\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LR = 0.001\n",
    "KL_weight = 0.1\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dkl\n",
    "\n",
    "from model import VAE\n",
    "\n",
    "from train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_kl, label=\"Train KL\")\n",
    "# plt.plot(val_kl, label=\"Val KL\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_recon, label=\"Train Recon\")\n",
    "# plt.plot(val_recon, label=\"Val Recon\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying DGMs to sc-RNA-seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathanlevy/mambaforge/envs/scvi/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/nathanlevy/mambaforge/envs/scvi/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/nathanlevy/mambaforge/envs/scvi/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/nathanlevy/mambaforge/envs/scvi/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "from anndata.experimental import AnnLoader\n",
    "\n",
    "import scvi\n",
    "import scanpy as sc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File .\u001b[35m/\u001b[0m\u001b[95mhca_subsampled_20k.h5ad\u001b[0m already downloaded                                                         \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">AnnData object with n_obs Ã— n_vars = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18641</span> Ã— <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26662</span>\n",
       "    obs: <span style=\"color: #008000; text-decoration-color: #008000\">'NRP'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'age_group'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_source'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_type'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'donor'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gender'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_counts'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_genes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'percent_mito'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'percent_ribo'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'region'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'scrublet_score'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Used'</span>\n",
       "    var: <span style=\"color: #008000; text-decoration-color: #008000\">'gene_ids-Harvard-Nuclei'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'feature_types-Harvard-Nuclei'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gene_ids-Sanger-Nuclei'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'feature_types-Sanger-Nuclei'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gene_ids-Sanger-Cells'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'feature_types-Sanger-Cells'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gene_ids-Sanger-CD45'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'feature_types-Sanger-CD45'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_counts'</span>\n",
       "    uns: <span style=\"color: #008000; text-decoration-color: #008000\">'cell_type_colors'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = \u001b[1;36m18641\u001b[0m Ã— \u001b[1;36m26662\u001b[0m\n",
       "    obs: \u001b[32m'NRP'\u001b[0m, \u001b[32m'age_group'\u001b[0m, \u001b[32m'cell_source'\u001b[0m, \u001b[32m'cell_type'\u001b[0m, \u001b[32m'donor'\u001b[0m, \u001b[32m'gender'\u001b[0m, \u001b[32m'n_counts'\u001b[0m, \u001b[32m'n_genes'\u001b[0m, \u001b[32m'percent_mito'\u001b[0m, \n",
       "\u001b[32m'percent_ribo'\u001b[0m, \u001b[32m'region'\u001b[0m, \u001b[32m'sample'\u001b[0m, \u001b[32m'scrublet_score'\u001b[0m, \u001b[32m'source'\u001b[0m, \u001b[32m'type'\u001b[0m, \u001b[32m'version'\u001b[0m, \u001b[32m'cell_states'\u001b[0m, \u001b[32m'Used'\u001b[0m\n",
       "    var: \u001b[32m'gene_ids-Harvard-Nuclei'\u001b[0m, \u001b[32m'feature_types-Harvard-Nuclei'\u001b[0m, \u001b[32m'gene_ids-Sanger-Nuclei'\u001b[0m, \n",
       "\u001b[32m'feature_types-Sanger-Nuclei'\u001b[0m, \u001b[32m'gene_ids-Sanger-Cells'\u001b[0m, \u001b[32m'feature_types-Sanger-Cells'\u001b[0m, \u001b[32m'gene_ids-Sanger-CD45'\u001b[0m, \n",
       "\u001b[32m'feature_types-Sanger-CD45'\u001b[0m, \u001b[32m'n_counts'\u001b[0m\n",
       "    uns: \u001b[32m'cell_type_colors'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adata = scvi.data.heart_cell_atlas_subsampled(save_path=\"./\")\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_counts=5)\n",
    "sc.pp.filter_cells(adata, min_counts=5)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "adata.raw = adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    n_top_genes=1200,\n",
    "    subset=True,\n",
    "    layer=\"counts\",\n",
    "    flavor=\"seurat_v3\",\n",
    "    batch_key=\"cell_source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">#train cells:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14912</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "#train cells:  \u001b[1;36m14912\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">#test cells:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3729</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "#test cells:  \u001b[1;36m3729\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_cells = len(adata)\n",
    "frac_train = 0.8\n",
    "\n",
    "train_index = np.random.choice(n_cells, size=int(frac_train * n_cells), replace=False)\n",
    "\n",
    "train_mask = np.empty(shape=(n_cells,), dtype=bool)\n",
    "\n",
    "train_mask[:] = False\n",
    "train_mask[train_index] = True\n",
    "test_mask = np.logical_not(train_mask)\n",
    "\n",
    "adata_train = adata[train_mask].copy()\n",
    "adata_test = adata[test_mask].copy()\n",
    "\n",
    "print(\"#train cells: \", len(adata_train))\n",
    "print(\"#test cells: \", len(adata_test))\n",
    "\n",
    "train_dataloader = AnnLoader(adata_train, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = AnnLoader(adata_test, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">AnnCollectionView object with n_obs Ã— n_vars = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span> Ã— <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1200</span>\n",
       "    layers: <span style=\"color: #008000; text-decoration-color: #008000\">'counts'</span>\n",
       "    obs: <span style=\"color: #008000; text-decoration-color: #008000\">'NRP'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'age_group'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_source'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_type'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'donor'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gender'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_counts'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_genes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'percent_mito'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'percent_ribo'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'region'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'scrublet_score'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cell_states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Used'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "AnnCollectionView object with n_obs Ã— n_vars = \u001b[1;36m128\u001b[0m Ã— \u001b[1;36m1200\u001b[0m\n",
       "    layers: \u001b[32m'counts'\u001b[0m\n",
       "    obs: \u001b[32m'NRP'\u001b[0m, \u001b[32m'age_group'\u001b[0m, \u001b[32m'cell_source'\u001b[0m, \u001b[32m'cell_type'\u001b[0m, \u001b[32m'donor'\u001b[0m, \u001b[32m'gender'\u001b[0m, \u001b[32m'n_counts'\u001b[0m, \u001b[32m'n_genes'\u001b[0m, \u001b[32m'percent_mito'\u001b[0m, \n",
       "\u001b[32m'percent_ribo'\u001b[0m, \u001b[32m'region'\u001b[0m, \u001b[32m'sample'\u001b[0m, \u001b[32m'scrublet_score'\u001b[0m, \u001b[32m'source'\u001b[0m, \u001b[32m'type'\u001b[0m, \u001b[32m'version'\u001b[0m, \u001b[32m'cell_states'\u001b[0m, \u001b[32m'Used'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print the first batch of the train_dataloader\n",
    "\n",
    "for x in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  0.,  0., ...,  0.,  0., 13.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 1.,  0.,  0., ...,  0.,  0., 12.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"counts\"].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=1200, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (mu): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (log_var): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder): PoissonDecoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (mean): Linear(in_features=128, out_features=1200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model = VAE(\n",
    "    adata.n_vars,\n",
    "    adata.n_vars,\n",
    "    hidden_sizes_encoder=[128],\n",
    "    hidden_sizes_decoder=[128],\n",
    "    latent_dim=LATENT_DIM,\n",
    "    likelihood=\"poisson\",\n",
    "    log_variational=True,\n",
    ")\n",
    "\n",
    "count_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    count_model.parameters(), lr=LR, momentum=0.9, weight_decay=1e-5\n",
    ")\n",
    "# optimizer = Lion(count_model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     count_model.parameters(), lr=LR\n",
    "# )\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (128, 10)) of distribution Normal(loc: torch.Size([128, 10]), scale: torch.Size([128, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_kl, train_recon, test_kl, test_recon \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs_kl_warmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_kl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_kl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TA/AI_hub/ex-vae/train.py:151\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, optimizer, scheduler, train_loader, test_loader, device, n_epochs, test_every, n_epochs_kl_warmup, max_kl_weight, min_kl_weight)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    144\u001b[0m     kl_weight \u001b[38;5;241m=\u001b[39m _compute_kl_weight(\n\u001b[1;32m    145\u001b[0m         epoch,\n\u001b[1;32m    146\u001b[0m         n_epochs_kl_warmup\u001b[38;5;241m=\u001b[39mn_epochs_kl_warmup,\n\u001b[1;32m    147\u001b[0m         max_kl_weight\u001b[38;5;241m=\u001b[39mmax_kl_weight,\n\u001b[1;32m    148\u001b[0m         min_kl_weight\u001b[38;5;241m=\u001b[39mmin_kl_weight,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     train_kl_epoch, train_recon_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m test_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TA/AI_hub/ex-vae/train.py:39\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, scheduler, kl_weight, loader, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m x_input \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m p_x, q_z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ------------------------------WRITE YOUR CODE---------------------------------#\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# compute the KL part of the ELBO loss here\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# kl_loss = dkl(q_z).sum(axis=1).mean()\u001b[39;00m\n\u001b[1;32m     45\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mkl_divergence(q_z, torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/scvi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/TA/AI_hub/ex-vae/model.py:65\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational:\n\u001b[1;32m     63\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m latent_dist, latent_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m obs_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(latent_sample)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_dist, latent_dist\n",
      "File \u001b[0;32m~/mambaforge/envs/scvi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/TA/AI_hub/ex-vae/base_components.py:72\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_var(x)\n\u001b[1;32m     70\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftplus()(log_var)\n\u001b[0;32m---> 72\u001b[0m latent_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m latent_sample \u001b[38;5;241m=\u001b[39m latent_dist\u001b[38;5;241m.\u001b[39mrsample()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latent_dist, latent_sample\n",
      "File \u001b[0;32m~/mambaforge/envs/scvi/lib/python3.11/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/scvi/lib/python3.11/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (128, 10)) of distribution Normal(loc: torch.Size([128, 10]), scale: torch.Size([128, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "train_kl, train_recon, test_kl, test_recon = train_loop(\n",
    "    model=count_model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    device=DEVICE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    test_every=1,\n",
    "    n_epochs_kl_warmup=10,\n",
    "    max_kl_weight=1.0,\n",
    "    min_kl_weight=0.0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenospace_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
